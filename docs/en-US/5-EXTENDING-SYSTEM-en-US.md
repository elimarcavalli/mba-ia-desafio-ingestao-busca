# üîå 5. Extending the System

Clean Architecture allows adding new providers without changing business rules.

This guide shows the complete step-by-step using **Anthropic Claude** as an example.

---

## üìã Overview

To add a new AI provider, you need to modify 5 files:

| #   | File                                               | Change               |
| --- | -------------------------------------------------- | -------------------- |
| 1   | `requirements.txt`                                 | Add dependency       |
| 2   | `src/infrastructure/adapters/`                     | Create new adapter   |
| 3   | `src/config/settings.py`                           | Add configurations   |
| 4   | `src/infrastructure/factories/provider_factory.py` | Register adapter     |
| 5   | `main.py`                                          | Add option to wizard |

---

## üöÄ Step by Step: Adding Anthropic Claude

### Step 1: Update requirements.txt

Add the LangChain dependency for Anthropic:

```text
# LangChain
langchain
langchain-core
langchain-community
langchain-text-splitters
langchain-postgres
langchain-openai
langchain-google-genai
langchain-anthropic  # <- ADD
```

---

### Step 2: Create the Adapter

Create the file `src/infrastructure/adapters/anthropic_llm.py`:

```python
"""
Anthropic LLM adapter.
Implements LLMPort for Anthropic Claude.
"""
from langchain_anthropic import ChatAnthropic

from src.config.settings import get_settings
from src.domain.ports.llm import LLMPort


class AnthropicLLMAdapter(LLMPort):
    """Anthropic Claude LLM adapter."""

    def __init__(self):
        settings = get_settings()
        self._llm = ChatAnthropic(
            model=settings.anthropic_model,
            anthropic_api_key=settings.anthropic_api_key,
            temperature=0,
            timeout=settings.llm_timeout,
        )

    def generate(self, prompt: str) -> str:
        """Generate response for prompt."""
        response = self._llm.invoke(prompt)
        return response.content

    def get_langchain_llm(self) -> ChatAnthropic:
        """Get LangChain LLM object."""
        return self._llm
```

---

### Step 3: Add Configurations

Edit `src/config/settings.py`:

```python
# Add "anthropic" to the llm_provider Literal
llm_provider: Literal["openai", "google", "anthropic"] = "openai"

# Add configurations after the Google section
# Anthropic
anthropic_api_key: str | None = None
anthropic_model: str = "claude-sonnet-4-5"
```

---

### Step 4: Register in the Factory

Edit `src/infrastructure/factories/provider_factory.py`:

**4.1. Add the import at the top:**

```python
from src.infrastructure.adapters.anthropic_llm import AnthropicLLMAdapter
```

**4.2. Add in the `get_embeddings()` method:**

> ‚ö†Ô∏è Anthropic doesn't have its own embeddings API, so we use OpenAI.

```python
elif settings.llm_provider == "anthropic":
    # Anthropic uses OpenAI embeddings
    if not settings.openai_api_key:
        raise ProviderNotConfiguredError("OpenAI API key required for embeddings when using Anthropic")
    cls._embeddings = OpenAIEmbeddingsAdapter()
```

**4.3. Add in the `get_llm()` method:**

```python
elif settings.llm_provider == "anthropic":
    if not settings.anthropic_api_key:
        raise ProviderNotConfiguredError("Anthropic API key not configured")
    cls._llm = AnthropicLLMAdapter()
```

---

### Step 5: Update the Wizard (main.py)

Edit the `configure_env()` function in `main.py`:

**5.1. Add option 3 to the menu:**

```python
while provider not in ["1", "2", "3"]:
    print("\nSelect your LLM Provider:")
    print("1. OpenAI (https://platform.openai.com/api-keys)")
    print("2. Google Gemini (https://aistudio.google.com/api-keys)")
    print("3. Anthropic Claude (https://platform.claude.com/settings/keys)")
    provider = input("Choice (1/2/3): ").strip()
```

**5.2. Update the selection logic:**

```python
if provider == "1":
    llm_provider = "openai"
elif provider == "2":
    llm_provider = "google"
else:
    llm_provider = "anthropic"
```

**5.3. Update the API key collection:**

```python
openai_api_key = ""
google_api_key = ""
anthropic_api_key = ""

if llm_provider == "openai":
    openai_api_key = input("Enter your OPENAI_API_KEY: ").strip()
elif llm_provider == "google":
    google_api_key = input("Enter your GOOGLE_API_KEY: ").strip()
else:  # anthropic
    anthropic_api_key = input("Enter your ANTHROPIC_API_KEY: ").strip()
    print_color("\nNote: Anthropic requires OpenAI for embeddings.", "WARNING")
    openai_api_key = input("Enter your OPENAI_API_KEY (for embeddings): ").strip()
```

**5.4. Update the .env template:**

```python
env_content = f"""# Generated by main.py
LLM_PROVIDER={llm_provider}

DATABASE_URL={db_base_url}
PG_VECTOR_COLLECTION_NAME=document_chunks
PDF_PATH=document.pdf

# API Keys
OPENAI_API_KEY={openai_api_key}
GOOGLE_API_KEY={google_api_key}
ANTHROPIC_API_KEY={anthropic_api_key}

# Models (Defaults)
OPENAI_EMBEDDING_MODEL='text-embedding-3-small'
GOOGLE_EMBEDDING_MODEL='models/embedding-001'
ANTHROPIC_MODEL='claude-sonnet-4-5'
"""
```

---

## üìã Available Claude Models

| Model               | Usage               |
| ------------------- | ------------------- |
| `claude-haiku-4-5`  | Fast and economical |
| `claude-sonnet-4-5` | Balanced            |
| `claude-opus-4-5`   | Most intelligent    |

---

## ‚úÖ Result

After completing all steps:

1. Run `python3 main.py`
2. Choose option 5 (Reset)
3. Choose option 1 (Start)
4. Select option 3 (Anthropic)
5. Enter the API keys

The system will use Claude for generation and OpenAI only for embeddings.

**Files that DON'T need to be changed:**

- `search_documents.py`
- `chainlit_app.py`
- Any domain tests

---

# ü§ñ Test the Scalability!

Want to see the power of Clean Architecture in action?

Ask your AI agent to implement Anthropic Claude support using only this document as reference:

```
Implement @docs/en-US/5-EXTENDING-SYSTEM.md
```

The agent will add Anthropic Claude support without needing additional explanations.

# üöÄ Done. Now you have three AI providers.

## üìö Other Available Providers

LangChain supports dozens of providers. Some popular ones:

| Provider         | Package                                                                                     |
| ---------------- | ------------------------------------------------------------------------------------------- |
| OpenAI           | [langchain-openai](https://python.langchain.com/docs/integrations/providers/openai)         |
| Google Gemini    | [langchain-google-genai](https://python.langchain.com/docs/integrations/providers/google)   |
| Anthropic Claude | [langchain-anthropic](https://python.langchain.com/docs/integrations/providers/anthropic)   |
| AWS Bedrock      | [langchain-aws](https://python.langchain.com/docs/integrations/providers/aws)               |
| Ollama (local)   | [langchain-ollama](https://python.langchain.com/docs/integrations/providers/ollama)         |
| Groq             | [langchain-groq](https://python.langchain.com/docs/integrations/providers/groq)             |
| MistralAI        | [langchain-mistralai](https://python.langchain.com/docs/integrations/providers/mistralai)   |
| Cohere           | [langchain-cohere](https://python.langchain.com/docs/integrations/providers/cohere)         |
| xAI (Grok)       | [langchain-xai](https://python.langchain.com/docs/integrations/providers/xai)               |
| DeepSeek         | [langchain-deepseek](https://python.langchain.com/docs/integrations/providers/deepseek)     |
| Perplexity       | [langchain-perplexity](https://python.langchain.com/docs/integrations/providers/perplexity) |
| Fireworks        | [langchain-fireworks](https://python.langchain.com/docs/integrations/providers/fireworks)   |

**Complete list:** [python.langchain.com/docs/integrations/chat](https://python.langchain.com/docs/integrations/chat/)

**For reference on how to implement each provider, access the respective LangChain documentation.**
